{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Validation Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input:\n",
    "* /data/validation.csv\n",
    "    * depends on \n",
    "        * /data/final-validation-set.rds\n",
    "        * data_context @ targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, cohen_kappa_score\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_curve, roc_auc_score\n",
    "from itertools import combinations\n",
    "from pingouin import intraclass_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/validation.csv\", encoding=\"ISO-8859-1\")\n",
    "for c in df.columns: print(c, end=\", \")\n",
    "print(\"\\n\\nSample Size: \" + str(df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base rates in human-coded tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.binary.value_counts())\n",
    "print(df.trinary.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SentiStrength sentiment strength scales to human coder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_kappa(name, x, y): print(name+\": \"+str(round(cohen_kappa_score(x,y), 3)))\n",
    "print_kappa(\"SentiStrength Pos\", df.pos, df.ss_pos)\n",
    "print_kappa(\"SentiStrength Neg\", df.neg, df.ss_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_icc(a, b):\n",
    "    df[\"row\"]=list(range(1, df.shape[0]+1))\n",
    "    df_icc_pos=df[[\"row\", a, b]]\n",
    "    df_icc_pos=pd.melt(df_icc_pos, id_vars=[\"row\"], value_vars=[a, b], var_name=\"rater\", value_name=\"res\")\n",
    "    icc=intraclass_corr(data=df_icc_pos, targets='row', raters='rater', ratings='res').round(3)\n",
    "    print(icc, end=\"\\n\\n\")\n",
    "\n",
    "print_icc(\"pos\", \"ss_pos\")\n",
    "print_icc(\"neg\", \"ss_neg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary and trinary classfications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_eval_binary(name, x, y):\n",
    "        \n",
    "    print(\"*** Classification Evaluation for: \" + name + \" ***\\n\")\n",
    "    cm = pd.DataFrame(\n",
    "        confusion_matrix(x, y),\n",
    "        index=['human:neg', 'human:pos'], \n",
    "        columns=['pred:neg', 'pred:pos']\n",
    "    )\n",
    "    print(cm, \"\\n\")\n",
    "    \n",
    "    print('accuracy:\\t{}'.format(round(accuracy_score(x, y) * 100, 2)), \"\\n\")    \n",
    "\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(x, y, labels=[1,0])\n",
    "    \n",
    "    print('\\t\\tpos\\tneg')\n",
    "    \n",
    "    print('precision:\\t{}\\t{}'.format(*[round(i, 2) for i in precision]))\n",
    "    print('recall:\\t\\t{}\\t{}'.format(*[round(i, 2) for i in recall]))\n",
    "    print('fscore:\\t\\t{}\\t{}'.format(*[round(i, 2) for i in fscore]))\n",
    "    print('support:\\t{}\\t{}'.format(*[round(i, 2) for i in support]))\n",
    "    print('')\n",
    "    \n",
    "print_eval_binary(\"SentiStrength\", df.binary, df.ss_binary)\n",
    "print_eval_binary(\"LIWC\", df.binary, df.liwc_binary)\n",
    "print_eval_binary(\"Bing\", df.binary, df.bing_binary)\n",
    "print_eval_binary(\"AFINN\", df.binary, df.afinn_binary)\n",
    "print_eval_binary(\"Loughran\", df.binary, df.loughran_binary)\n",
    "print_eval_binary(\"NRC\", df.binary, df.nrc_binary)\n",
    "print_eval_binary(\"Tidytext\", df.binary, df.tidytext_binary)\n",
    "print_eval_binary(\"VADER\", df.binary, df.vader_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_eval_binary_pair(a, b, x, y):\n",
    "    print(\"*** Pairwise comparison: \" + a + \" + \" + b + \" ***\\n\")\n",
    "    print('accuracy:\\t{}'.format(round(accuracy_score(x, y) * 100, 2)), \"\\n\") \n",
    "\n",
    "for a, b in list(combinations([\"ss_binary\", \"liwc_binary\", \"tidytext_binary\", \"vader_binary\"], 2)):\n",
    "    print_eval_binary_pair(a, b, df[a], df[b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_eval_trinary(name, x, y):\n",
    "        \n",
    "    print(\"*** Classification Evaluation for: \" + name + \" ***\\n\")\n",
    "    cm = pd.DataFrame(\n",
    "        confusion_matrix(x, y),\n",
    "        index=['human:neg', 'human:neutral', 'human:pos'],\n",
    "        columns=['pred:neg', 'pred:neutral', 'pred:pos']\n",
    "    )\n",
    "    print(cm, \"\\n\")\n",
    "    \n",
    "    print('accuracy:\\t{}'.format(round(accuracy_score(x, y) * 100, 2)), \"\\n\")    \n",
    "\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(x, y, labels=[1,0,-1])\n",
    "\n",
    "    print('\\t\\tpos\\tneut\\tneg')\n",
    "    print('precision:\\t{}\\t{}\\t{}'.format(*[round(i, 2) for i in precision]))\n",
    "    print('recall:\\t\\t{}\\t{}\\t{}'.format(*[round(i, 2) for i in recall]))\n",
    "    print('fscore:\\t\\t{}\\t{}\\t{}'.format(*[round(i, 2) for i in fscore]))\n",
    "    print('support:\\t{}\\t{}\\t{}'.format(*[round(i, 2) for i in support]), \"\\n\")\n",
    "\n",
    "print_eval_trinary(\"SentiStrength\", df.trinary, df.ss_trinary)\n",
    "print_eval_trinary(\"LIWC\", df.trinary, df.liwc_trinary)\n",
    "print_eval_trinary(\"Bing\", df.trinary, df.bing_trinary)\n",
    "print_eval_trinary(\"AFINN\", df.trinary, df.afinn_trinary)\n",
    "print_eval_trinary(\"Loughran\", df.trinary, df.loughran_trinary)\n",
    "print_eval_trinary(\"NRC\", df.trinary, df.nrc_trinary)\n",
    "print_eval_trinary(\"Tidytext\", df.trinary, df.tidytext_trinary)\n",
    "print_eval_trinary(\"VADER\", df.trinary, df.vader_trinary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_eval_trinary_pair(a, b, x, y):\n",
    "        \n",
    "    print(\"*** Pairwise comparison: \" + a + \" + \" + b + \" ***\\n\")\n",
    "    \n",
    "    print('accuracy:\\t{}'.format(round(accuracy_score(x, y) * 100, 2)), \"\\n\")\n",
    "    \n",
    "for a, b in list(combinations([\"ss_trinary\", \"liwc_trinary\", \"tidytext_trinary\", \"vader_trinary\"], 2)):\n",
    "    print_eval_binary_pair(a, b, df[a], df[b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sinfo import sinfo\n",
    "sinfo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
