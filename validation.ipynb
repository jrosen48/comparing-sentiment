{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Validation Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input:\n",
    "* /data/validation.csv\n",
    "    * depends on \n",
    "        * /data/final-validation-set.rds\n",
    "        * data_context @ targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, cohen_kappa_score\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_curve, roc_auc_score\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/validation.csv\", encoding=\"ISO-8859-1\")\n",
    "for c in df.columns: print(c, end=\", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SentiStrength sentiment strength scales to human coder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_kappa(name, x, y): print(name+\": \"+str(round(cohen_kappa_score(x,y), 3)))\n",
    "print_kappa(\"SentiStrength Pos\", df.pos, df.ss_pos)\n",
    "print_kappa(\"SentiStrength Neg\", df.neg, df.ss_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary and trinary classfications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_eval_binary(name, x, y):\n",
    "        \n",
    "    print(\"*** Classification Evaluation for: \" + name + \" ***\\n\")\n",
    "    cm = pd.DataFrame(\n",
    "        confusion_matrix(x, y),\n",
    "        index=['human:neg', 'human:pos'], \n",
    "        columns=['pred:neg', 'pred:pos']\n",
    "    )\n",
    "    print(cm, \"\\n\")\n",
    "    \n",
    "    print('accuracy:\\t{}'.format(round(accuracy_score(x, y) * 100, 2)), \"\\n\")    \n",
    "\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(x, y, labels=[1,0])\n",
    "    \n",
    "    print('\\t\\tpos\\tneg')\n",
    "    \n",
    "    print('precision:\\t{}\\t{}'.format(*[round(i, 2) for i in precision]))\n",
    "    print('recall:\\t\\t{}\\t{}'.format(*[round(i, 2) for i in recall]))\n",
    "    print('fscore:\\t\\t{}\\t{}'.format(*[round(i, 2) for i in fscore]))\n",
    "    print('support:\\t{}\\t{}'.format(*[round(i, 2) for i in support]))\n",
    "    print('')\n",
    "    \n",
    "print_eval_binary(\"SentiStrength\", df.binary, df.ss_binary)\n",
    "print_eval_binary(\"LIWC\", df.binary, df.liwc_binary)\n",
    "print_eval_binary(\"Bing\", df.binary, df.bing_binary)\n",
    "print_eval_binary(\"AFINN\", df.binary, df.afinn_binary)\n",
    "print_eval_binary(\"Loughran\", df.binary, df.loughran_binary)\n",
    "print_eval_binary(\"NRC\", df.binary, df.nrc_binary)\n",
    "print_eval_binary(\"Tidytext\", df.binary, df.tidytext_binary)\n",
    "print_eval_binary(\"VADER\", df.binary, df.vader_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_eval_binary_pair(a, b, x, y):\n",
    "    print(\"*** Pairwise comparison: \" + a + \" + \" + b + \" ***\\n\")\n",
    "    print('accuracy:\\t{}'.format(round(accuracy_score(x, y) * 100, 2)), \"\\n\") \n",
    "\n",
    "for a, b in list(combinations([\"ss_binary\", \"liwc_binary\", \"tidytext_binary\", \"vader_binary\"], 2)):\n",
    "    print_eval_binary_pair(a, b, df[a], df[b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_eval_trinary(name, x, y):\n",
    "        \n",
    "    print(\"*** Classification Evaluation for: \" + name + \" ***\\n\")\n",
    "    cm = pd.DataFrame(\n",
    "        confusion_matrix(x, y),\n",
    "        index=['human:neg', 'human:neutral', 'human:pos'],\n",
    "        columns=['pred:neg', 'pred:neutral', 'pred:pos']\n",
    "    )\n",
    "    print(cm, \"\\n\")\n",
    "    \n",
    "    print('accuracy:\\t{}'.format(round(accuracy_score(x, y) * 100, 2)), \"\\n\")    \n",
    "\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(x, y, labels=[1,0,-1])\n",
    "\n",
    "    print('\\t\\tpos\\tneut\\tneg')\n",
    "    print('precision:\\t{}\\t{}\\t{}'.format(*[round(i, 2) for i in precision]))\n",
    "    print('recall:\\t\\t{}\\t{}\\t{}'.format(*[round(i, 2) for i in recall]))\n",
    "    print('fscore:\\t\\t{}\\t{}\\t{}'.format(*[round(i, 2) for i in fscore]))\n",
    "    print('support:\\t{}\\t{}\\t{}'.format(*[round(i, 2) for i in support]), \"\\n\")\n",
    "\n",
    "print_eval_trinary(\"SentiStrength\", df.trinary, df.ss_trinary)\n",
    "print_eval_trinary(\"LIWC\", df.trinary, df.liwc_trinary)\n",
    "print_eval_trinary(\"Bing\", df.trinary, df.bing_trinary)\n",
    "print_eval_trinary(\"AFINN\", df.trinary, df.afinn_trinary)\n",
    "print_eval_trinary(\"Loughran\", df.trinary, df.loughran_trinary)\n",
    "print_eval_trinary(\"NRC\", df.trinary, df.nrc_trinary)\n",
    "print_eval_trinary(\"Tidytext\", df.trinary, df.tidytext_trinary)\n",
    "print_eval_trinary(\"VADER\", df.trinary, df.vader_trinary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_eval_trinary_pair(a, b, x, y):\n",
    "        \n",
    "    print(\"*** Pairwise comparison: \" + a + \" + \" + b + \" ***\\n\")\n",
    "    \n",
    "    print('accuracy:\\t{}'.format(round(accuracy_score(x, y) * 100, 2)), \"\\n\")\n",
    "    \n",
    "for a, b in list(combinations([\"ss_trinary\", \"liwc_trinary\", \"tidytext_trinary\", \"vader_trinary\"], 2)):\n",
    "    print_eval_binary_pair(a, b, df[a], df[b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "numpy       1.19.2\n",
      "pandas      0.25.0\n",
      "sinfo       0.3.1\n",
      "sklearn     0.21.2\n",
      "-----\n",
      "IPython             7.19.0\n",
      "jupyter_client      6.1.7\n",
      "jupyter_core        4.7.0\n",
      "notebook            6.1.4\n",
      "-----\n",
      "Python 3.7.0 (default, Jun 28 2018, 08:04:48) [MSC v.1912 64 bit (AMD64)]\n",
      "Windows-10-10.0.18362-SP0\n",
      "8 logical CPU cores, Intel64 Family 6 Model 142 Stepping 11, GenuineIntel\n",
      "-----\n",
      "Session information updated at 2021-01-23 19:00\n"
     ]
    }
   ],
   "source": [
    "from sinfo import sinfo\n",
    "sinfo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
