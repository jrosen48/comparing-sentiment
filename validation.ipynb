{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix A: Validation Python Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input:\n",
    "* /data/validation.csv\n",
    "* /data/rater-icc-data.csv\n",
    "\n",
    "Depends on:\n",
    "* /data/validation-final.rds\n",
    "* data_context @ targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, cohen_kappa_score\n",
    "from sklearn.metrics import precision_recall_fscore_support, f1_score\n",
    "from pingouin import intraclass_corr\n",
    "import random\n",
    "import itertools\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Size: 300\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/validation.csv\", encoding=\"ISO-8859-1\")\n",
    "print(\"Sample Size: \" + str(df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base rates in human-coded tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    265\n",
      "0     35\n",
      "Name: binary, dtype: int64\n",
      " 0    150\n",
      " 1    115\n",
      "-1     35\n",
      "Name: trinary, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.binary.value_counts())\n",
    "print(df.trinary.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raters = pd.read_csv(\"data/rater-icc-data.csv\", encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>r1_pos</th>\n",
       "      <th>r2_pos</th>\n",
       "      <th>r1_neg</th>\n",
       "      <th>r2_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>67</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>68</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>69</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  r1_pos  r2_pos  r1_neg  r2_neg\n",
       "0            1       1       1       1       1\n",
       "1            2       1       1       1       2\n",
       "2            3       1       1       1       1\n",
       "3            4       2       2       1       1\n",
       "4            5       3       3       1       1\n",
       "..         ...     ...     ...     ...     ...\n",
       "65          66       1       1       2       2\n",
       "66          67       2       2       1       1\n",
       "67          68       3       2       1       1\n",
       "68          69       4       3       1       1\n",
       "69          70       3       2       1       1\n",
       "\n",
       "[70 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SentiStrength sentiment strength scales to human coder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addition: Evaluate overall scale\n",
    "df[\"ss_scale\"] = df.ss_pos - df.ss_neg\n",
    "df[\"scale\"] = df.pos - df.neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      2\n",
       "1      1\n",
       "2      1\n",
       "3      2\n",
       "4      2\n",
       "      ..\n",
       "295    1\n",
       "296    2\n",
       "297    1\n",
       "298    1\n",
       "299    1\n",
       "Name: pos, Length: 300, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentiStrength Pos: 0.301\n",
      "SentiStrength Neg: 0.183\n",
      "SentiStrength Overall: 0.27\n"
     ]
    }
   ],
   "source": [
    "def print_kappa(name, x, y): print(name+\": \"+str(round(cohen_kappa_score(x,y), 3)))\n",
    "print_kappa(\"SentiStrength Pos\", df.pos, df.ss_pos)\n",
    "print_kappa(\"SentiStrength Neg\", df.neg, df.ss_neg)\n",
    "print_kappa(\"SentiStrength Overall\", df.scale, df.ss_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.87315036],\n",
       "       [0.87315036, 1.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(df.pos, df.scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Type              Description    ICC      F  df1  df2  pval         CI95%\n",
      "0   ICC1   Single raters absolute  0.513  3.108  299  300   0.0  [0.42, 0.59]\n",
      "1   ICC2     Single random raters  0.527  3.519  299  299   0.0   [0.4, 0.63]\n",
      "2   ICC3      Single fixed raters  0.557  3.519  299  299   0.0  [0.47, 0.63]\n",
      "3  ICC1k  Average raters absolute  0.678  3.108  299  300   0.0   [0.6, 0.74]\n",
      "4  ICC2k    Average random raters  0.690  3.519  299  299   0.0  [0.57, 0.77]\n",
      "5  ICC3k     Average fixed raters  0.716  3.519  299  299   0.0  [0.64, 0.77]\n",
      "\n",
      "    Type              Description    ICC      F  df1  df2  pval         CI95%\n",
      "0   ICC1   Single raters absolute  0.287  1.806  299  300   0.0  [0.18, 0.39]\n",
      "1   ICC2     Single random raters  0.288  1.814  299  299   0.0  [0.18, 0.39]\n",
      "2   ICC3      Single fixed raters  0.289  1.814  299  299   0.0  [0.18, 0.39]\n",
      "3  ICC1k  Average raters absolute  0.446  1.806  299  300   0.0  [0.31, 0.56]\n",
      "4  ICC2k    Average random raters  0.448  1.814  299  299   0.0  [0.31, 0.56]\n",
      "5  ICC3k     Average fixed raters  0.449  1.814  299  299   0.0  [0.31, 0.56]\n",
      "\n",
      "    Type              Description    ICC      F  df1  df2  pval         CI95%\n",
      "0   ICC1   Single raters absolute  0.513  3.103  299  300   0.0  [0.42, 0.59]\n",
      "1   ICC2     Single random raters  0.519  3.281  299  299   0.0   [0.42, 0.6]\n",
      "2   ICC3      Single fixed raters  0.533  3.281  299  299   0.0  [0.45, 0.61]\n",
      "3  ICC1k  Average raters absolute  0.678  3.103  299  300   0.0   [0.6, 0.74]\n",
      "4  ICC2k    Average random raters  0.683  3.281  299  299   0.0  [0.59, 0.75]\n",
      "5  ICC3k     Average fixed raters  0.695  3.281  299  299   0.0  [0.62, 0.76]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_icc(a, b):\n",
    "    df[\"row\"]=list(range(1, df.shape[0]+1))\n",
    "    df_icc_pos=df[[\"row\", a, b]]\n",
    "    df_icc_pos=pd.melt(df_icc_pos, id_vars=[\"row\"], value_vars=[a, b], \n",
    "                       var_name=\"rater\", value_name=\"res\")\n",
    "    icc=intraclass_corr(data=df_icc_pos, targets='row', raters='rater', \n",
    "                        ratings='res').round(3)\n",
    "    print(icc, end=\"\\n\\n\")\n",
    "\n",
    "print_icc(\"pos\", \"ss_pos\")\n",
    "print_icc(\"neg\", \"ss_neg\")\n",
    "print_icc(\"scale\", \"ss_scale\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary and trinary classfications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Classification Evaluation for: SentiStrength ***\n",
      "\n",
      "           pred:neg  pred:pos\n",
      "human:neg        14        21\n",
      "human:pos        24       241 \n",
      "\n",
      "accuracy:\t85.0 \n",
      "\n",
      "\t\tpos\tneg\n",
      "precision:\t0.92\t0.37\n",
      "recall:\t\t0.91\t0.4\n",
      "fscore:\t\t0.91\t0.38\n",
      "support:\t265\t35\n",
      "\n",
      "*** Classification Evaluation for: LIWC ***\n",
      "\n",
      "           pred:neg  pred:pos\n",
      "human:neg         7        28\n",
      "human:pos         7       258 \n",
      "\n",
      "accuracy:\t88.33 \n",
      "\n",
      "\t\tpos\tneg\n",
      "precision:\t0.9\t0.5\n",
      "recall:\t\t0.97\t0.2\n",
      "fscore:\t\t0.94\t0.29\n",
      "support:\t265\t35\n",
      "\n",
      "*** Classification Evaluation for: Tidytext ***\n",
      "\n",
      "           pred:neg  pred:pos\n",
      "human:neg        16        19\n",
      "human:pos        20       245 \n",
      "\n",
      "accuracy:\t87.0 \n",
      "\n",
      "\t\tpos\tneg\n",
      "precision:\t0.93\t0.44\n",
      "recall:\t\t0.92\t0.46\n",
      "fscore:\t\t0.93\t0.45\n",
      "support:\t265\t35\n",
      "\n",
      "*** Classification Evaluation for: VADER ***\n",
      "\n",
      "           pred:neg  pred:pos\n",
      "human:neg        14        21\n",
      "human:pos        14       251 \n",
      "\n",
      "accuracy:\t88.33 \n",
      "\n",
      "\t\tpos\tneg\n",
      "precision:\t0.92\t0.5\n",
      "recall:\t\t0.95\t0.4\n",
      "fscore:\t\t0.93\t0.44\n",
      "support:\t265\t35\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_eval_binary(name, x, y):\n",
    "        \n",
    "    print(\"*** Classification Evaluation for: \" + name + \" ***\\n\")\n",
    "    cm = pd.DataFrame(\n",
    "        confusion_matrix(x, y),\n",
    "        index=['human:neg', 'human:pos'], \n",
    "        columns=['pred:neg', 'pred:pos']\n",
    "    )\n",
    "    print(cm, \"\\n\")\n",
    "    \n",
    "    print('accuracy:\\t{}'.format(round(accuracy_score(x, y) * 100, 2)), \"\\n\")    \n",
    "\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(x, y, \n",
    "                                                                 labels=[1,0])\n",
    "    \n",
    "    print('\\t\\tpos\\tneg')\n",
    "    \n",
    "    print('precision:\\t{}\\t{}'.format(*[round(i, 2) for i in precision]))\n",
    "    print('recall:\\t\\t{}\\t{}'.format(*[round(i, 2) for i in recall]))\n",
    "    print('fscore:\\t\\t{}\\t{}'.format(*[round(i, 2) for i in fscore]))\n",
    "    print('support:\\t{}\\t{}'.format(*[round(i, 2) for i in support]))\n",
    "    print('')\n",
    "    \n",
    "print_eval_binary(\"SentiStrength\", df.binary, df.ss_binary)\n",
    "print_eval_binary(\"LIWC\", df.binary, df.liwc_binary)\n",
    "print_eval_binary(\"Tidytext\", df.binary, df.tidytext_binary)\n",
    "print_eval_binary(\"VADER\", df.binary, df.vader_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Classification Evaluation for: SentiStrength ***\n",
      "\n",
      "               pred:neg  pred:neutral  pred:pos\n",
      "human:neg            14            15         6\n",
      "human:neutral        15            95        40\n",
      "human:pos            10            16        89 \n",
      "\n",
      "accuracy:\t66.0 \n",
      "\n",
      "\t\tpos\tneut\tneg\n",
      "precision:\t0.66\t0.75\t0.36\n",
      "recall:\t\t0.77\t0.63\t0.4\n",
      "fscore:\t\t0.71\t0.69\t0.38\n",
      "support:\t115\t150\t35 \n",
      "\n",
      "*** Classification Evaluation for: LIWC ***\n",
      "\n",
      "               pred:neg  pred:neutral  pred:pos\n",
      "human:neg             7            21         7\n",
      "human:neutral         4           104        42\n",
      "human:pos             3            22        90 \n",
      "\n",
      "accuracy:\t67.0 \n",
      "\n",
      "\t\tpos\tneut\tneg\n",
      "precision:\t0.65\t0.71\t0.5\n",
      "recall:\t\t0.78\t0.69\t0.2\n",
      "fscore:\t\t0.71\t0.7\t0.29\n",
      "support:\t115\t150\t35 \n",
      "\n",
      "*** Classification Evaluation for: Tidytext ***\n",
      "\n",
      "               pred:neg  pred:neutral  pred:pos\n",
      "human:neg            16             2        17\n",
      "human:neutral        13            50        87\n",
      "human:pos             7             5       103 \n",
      "\n",
      "accuracy:\t56.33 \n",
      "\n",
      "\t\tpos\tneut\tneg\n",
      "precision:\t0.5\t0.88\t0.44\n",
      "recall:\t\t0.9\t0.33\t0.46\n",
      "fscore:\t\t0.64\t0.48\t0.45\n",
      "support:\t115\t150\t35 \n",
      "\n",
      "*** Classification Evaluation for: VADER ***\n",
      "\n",
      "               pred:neg  pred:neutral  pred:pos\n",
      "human:neg            14            10        11\n",
      "human:neutral         8            85        57\n",
      "human:pos             6            12        97 \n",
      "\n",
      "accuracy:\t65.33 \n",
      "\n",
      "\t\tpos\tneut\tneg\n",
      "precision:\t0.59\t0.79\t0.5\n",
      "recall:\t\t0.84\t0.57\t0.4\n",
      "fscore:\t\t0.69\t0.66\t0.44\n",
      "support:\t115\t150\t35 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_eval_trinary(name, x, y):\n",
    "        \n",
    "    print(\"*** Classification Evaluation for: \" + name + \" ***\\n\")\n",
    "    cm = pd.DataFrame(\n",
    "        confusion_matrix(x, y),\n",
    "        index=['human:neg', 'human:neutral', 'human:pos'],\n",
    "        columns=['pred:neg', 'pred:neutral', 'pred:pos']\n",
    "    )\n",
    "    print(cm, \"\\n\")\n",
    "    \n",
    "    print('accuracy:\\t{}'.format(round(accuracy_score(x, y) * 100, 2)), \"\\n\")    \n",
    "\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(x, y, \n",
    "                                                              labels=[1,0,-1])\n",
    "\n",
    "    print('\\t\\tpos\\tneut\\tneg')\n",
    "    print('precision:\\t{}\\t{}\\t{}'.format(*[round(i, 2) for i in precision]))\n",
    "    print('recall:\\t\\t{}\\t{}\\t{}'.format(*[round(i, 2) for i in recall]))\n",
    "    print('fscore:\\t\\t{}\\t{}\\t{}'.format(*[round(i, 2) for i in fscore]))\n",
    "    print('support:\\t{}\\t{}\\t{}'.format(*[round(i, 2) for i in support]), \"\\n\")\n",
    "\n",
    "print_eval_trinary(\"SentiStrength\", df.trinary, df.ss_trinary)\n",
    "print_eval_trinary(\"LIWC\", df.trinary, df.liwc_trinary)\n",
    "print_eval_trinary(\"Tidytext\", df.trinary, df.tidytext_trinary)\n",
    "print_eval_trinary(\"VADER\", df.trinary, df.vader_trinary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding statistical tests to validation results\n",
    "\n",
    "Reference: \n",
    "> Yeh, A. (2000). More accurate tests for the statistical significance of result differences. arXiv preprint cs/0008005.\n",
    "\n",
    "Null Hypothesis: There is no difference in F-score between two methods\n",
    "\n",
    "1. Take predictions of the methods\n",
    "2. Randomly assign them to any of the two measures and observe the difference in F-score\n",
    "3. Count how often that random difference is larger than the observed difference in F-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2021-04-22 12:52:57.666772\n",
      "\n",
      "Two-sided randomization test of two F-scores, n = 250000 \n",
      "\n",
      "H0: F-score of ss_binary is equal to F-score of liwc_binary\n",
      "H1: F-score of ss_binary is not equal to F-score of liwc_binary\n",
      "\n",
      "p: {1: 0.035936, 0: 0.225316}\n",
      "\n",
      "Time: 2021-04-22 12:59:16.226497\n",
      "\n",
      "Two-sided randomization test of two F-scores, n = 250000 \n",
      "\n",
      "H0: F-score of ss_binary is equal to F-score of tidytext_binary\n",
      "H1: F-score of ss_binary is not equal to F-score of tidytext_binary\n",
      "\n",
      "p: {1: 0.3903, 0: 0.366376}\n",
      "\n",
      "Time: 2021-04-22 13:05:50.533669\n",
      "\n",
      "Two-sided randomization test of two F-scores, n = 250000 \n",
      "\n",
      "H0: F-score of ss_binary is equal to F-score of vader_binary\n",
      "H1: F-score of ss_binary is not equal to F-score of vader_binary\n",
      "\n",
      "p: {1: 0.069604, 0: 0.340444}\n",
      "\n",
      "Time: 2021-04-22 13:12:31.594536\n",
      "\n",
      "Two-sided randomization test of two F-scores, n = 250000 \n",
      "\n",
      "H0: F-score of liwc_binary is equal to F-score of tidytext_binary\n",
      "H1: F-score of liwc_binary is not equal to F-score of tidytext_binary\n",
      "\n",
      "p: {1: 0.31972, 0: 0.057568}\n",
      "\n",
      "Time: 2021-04-22 13:19:15.585646\n",
      "\n",
      "Two-sided randomization test of two F-scores, n = 250000 \n",
      "\n",
      "H0: F-score of liwc_binary is equal to F-score of vader_binary\n",
      "H1: F-score of liwc_binary is not equal to F-score of vader_binary\n",
      "\n",
      "p: {1: 0.8241, 0: 0.047264}\n",
      "\n",
      "Time: 2021-04-22 13:25:51.003430\n",
      "\n",
      "Two-sided randomization test of two F-scores, n = 250000 \n",
      "\n",
      "H0: F-score of tidytext_binary is equal to F-score of vader_binary\n",
      "H1: F-score of tidytext_binary is not equal to F-score of vader_binary\n",
      "\n",
      "p: {1: 0.322576, 0: 0.938052}\n",
      "\n",
      "Time: 2021-04-22 13:32:25.052293\n",
      "\n",
      "Two-sided randomization test of two F-scores, n = 250000 \n",
      "\n",
      "H0: F-score of ss_trinary is equal to F-score of liwc_trinary\n",
      "H1: F-score of ss_trinary is not equal to F-score of liwc_trinary\n",
      "\n",
      "p: {1: 0.932044, 0: 0.729344, -1: 0.207528}\n",
      "\n",
      "Time: 2021-04-22 13:39:06.803332\n",
      "\n",
      "Two-sided randomization test of two F-scores, n = 250000 \n",
      "\n",
      "H0: F-score of ss_trinary is equal to F-score of tidytext_trinary\n",
      "H1: F-score of ss_trinary is not equal to F-score of tidytext_trinary\n",
      "\n",
      "p: {1: 0.025792, 0: 4e-05, -1: 0.323304}\n",
      "\n",
      "Time: 2021-04-22 13:45:47.723459\n",
      "\n",
      "Two-sided randomization test of two F-scores, n = 250000 \n",
      "\n",
      "H0: F-score of ss_trinary is equal to F-score of vader_trinary\n",
      "H1: F-score of ss_trinary is not equal to F-score of vader_trinary\n",
      "\n",
      "p: {1: 0.58466, 0: 0.506248, -1: 0.297044}\n",
      "\n",
      "Time: 2021-04-22 13:52:31.801425\n",
      "\n",
      "Two-sided randomization test of two F-scores, n = 250000 \n",
      "\n",
      "H0: F-score of liwc_trinary is equal to F-score of tidytext_trinary\n",
      "H1: F-score of liwc_trinary is not equal to F-score of tidytext_trinary\n",
      "\n",
      "p: {1: 0.010524, 0: 0.0, -1: 0.057084}\n",
      "\n",
      "Time: 2021-04-22 13:59:08.530207\n",
      "\n",
      "Two-sided randomization test of two F-scores, n = 250000 \n",
      "\n",
      "H0: F-score of liwc_trinary is equal to F-score of vader_trinary\n",
      "H1: F-score of liwc_trinary is not equal to F-score of vader_trinary\n",
      "\n",
      "p: {1: 0.519756, 0: 0.152012, -1: 0.04728}\n",
      "\n",
      "Time: 2021-04-22 14:05:50.050010\n",
      "\n",
      "Two-sided randomization test of two F-scores, n = 250000 \n",
      "\n",
      "H0: F-score of tidytext_trinary is equal to F-score of vader_trinary\n",
      "H1: F-score of tidytext_trinary is not equal to F-score of vader_trinary\n",
      "\n",
      "p: {1: 0.025212, 0: 1.6e-05, -1: 0.938184}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def simulate_fs(df, reference, col1, col2, n, two_sided=True):\n",
    "    print('Time:', datetime.datetime.now())\n",
    "    if reference == 'binary':\n",
    "        labels = [1, 0]\n",
    "    elif reference == 'trinary':\n",
    "        labels = [1, 0, -1]\n",
    "    else:\n",
    "        print('Please provice binary or trinary as reference.')\n",
    "        return False\n",
    "    if not two_sided:\n",
    "        print('\\nOne-sided randomization test of two F-scores, n =', n, '\\n')\n",
    "        print('H0: F-score of', col1, 'is equal or smaller than F-score of', col2)\n",
    "        print('H1: F-score of', col1, 'is larger than F-score of', col2, end='\\n\\np: ')\n",
    "    else:\n",
    "        print('\\nTwo-sided randomization test of two F-scores, n =', n, '\\n')\n",
    "        print('H0: F-score of', col1, 'is equal to F-score of', col2)\n",
    "        print('H1: F-score of', col1, 'is not equal to F-score of', col2, end='\\n\\np: ')\n",
    "    _, _, fscores1, _ = precision_recall_fscore_support(df[reference], df[col1], \n",
    "                                                        labels=labels)\n",
    "    _, _, fscores2, _ = precision_recall_fscore_support(df[reference], df[col2], \n",
    "                                                        labels=labels)\n",
    "    diffs = [a-b for a, b in zip(fscores1, fscores2)]\n",
    "    diffsdict = dict(zip(labels, diffs))\n",
    "    simulated = {}\n",
    "    for label in labels: \n",
    "        simulated[label] = []\n",
    "    for _ in range(n):\n",
    "        shuffle1 = []\n",
    "        shuffle2 = []\n",
    "        for a, b in zip(df[col1], df[col2]):\n",
    "            if random.randint(0, 1) == 1:\n",
    "                shuffle1.append(a)\n",
    "                shuffle2.append(b)\n",
    "            else:\n",
    "                shuffle1.append(b)\n",
    "                shuffle2.append(a) \n",
    "        _, _, fscores1, _ = precision_recall_fscore_support(df[reference], shuffle1, \n",
    "                                                           labels=labels)\n",
    "        _, _, fscores2, _ = precision_recall_fscore_support(df[reference], shuffle2, \n",
    "                                                           labels=labels)\n",
    "        diffs = [a-b for a, b in zip(fscores1, fscores2)]\n",
    "        for label, diff in zip(labels, diffs):\n",
    "            simulated[label].append(diff)\n",
    "    p = {}\n",
    "    for label in labels:\n",
    "        if not two_sided:\n",
    "            overdiff = [d for d in simulated[label] if d >= diffsdict[label]]\n",
    "        else:\n",
    "            overdiff = [d for d in simulated[label] if abs(d) >= abs(diffsdict[label])]\n",
    "        p[label] = len(overdiff)/n\n",
    "    return p\n",
    "\n",
    "\n",
    "for a, b in itertools.combinations(['ss_binary', 'liwc_binary', 'tidytext_binary', \n",
    "                        'vader_binary'], 2):\n",
    "    print(simulate_fs(df, 'binary', a, b, 250000, two_sided=True), end='\\n\\n')\n",
    "    \n",
    "for a, b in itertools.combinations(['ss_trinary', 'liwc_trinary', 'tidytext_trinary', \n",
    "                        'vader_trinary'], 2):\n",
    "    print(simulate_fs(df, 'trinary', a, b, 250000, two_sided=True), end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "numpy       1.20.2\n",
      "pandas      1.2.4\n",
      "pingouin    0.3.11\n",
      "sinfo       0.3.1\n",
      "sklearn     0.24.1\n",
      "-----\n",
      "IPython             7.22.0\n",
      "jupyter_client      6.1.12\n",
      "jupyter_core        4.7.1\n",
      "notebook            6.3.0\n",
      "-----\n",
      "Python 3.8.5 (default, Jan 27 2021, 15:41:15) [GCC 9.3.0]\n",
      "Linux-5.8.0-50-generic-x86_64-with-glibc2.29\n",
      "16 logical CPU cores, x86_64\n",
      "-----\n",
      "Session information updated at 2021-04-22 14:12\n"
     ]
    }
   ],
   "source": [
    "from sinfo import sinfo\n",
    "sinfo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
